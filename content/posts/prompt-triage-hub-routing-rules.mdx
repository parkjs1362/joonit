---
title: "Prompt Triage Hub 설계: 작업 유형별 AI 라우팅 규칙"
description: "모든 질문을 한 모델에 던지면 반복 수정이 늘어납니다. 작업 유형별 분류로 프롬프트 라우팅을 단순화하는 방법을 정리합니다."
date: "2026-02-17"
category: "개발"
tags: ["프롬프트", "AI", "라우팅", "워크플로우"]
---

AI 도구를 여러 개 쓰면 생산성이 오를 것 같지만, 실제로는 선택 비용이 늘기 쉽습니다.  
어떤 모델을 열지 고민하다가 작업 흐름이 끊기는 순간이 자주 생깁니다.

이 문제를 줄이기 위해 저는 **Prompt Triage Hub**를 먼저 둡니다.  
핵심은 모델 선택이 아니라, **작업 유형을 먼저 분류**하는 것입니다.

## 1) 분류 축은 4개면 충분하다

처음부터 복잡한 taxonomy를 만들 필요는 없습니다.

- code: 구현/리팩터링/설계
- debug: 오류 재현/원인 분석
- write: 문서/메일/정리
- research: 비교/자료 조사

이 4개만 고정해도 라우팅 품질이 안정됩니다.

## 2) 프롬프트 템플릿을 유형별로 분리한다

같은 질문도 유형에 따라 출력 형식이 달라야 합니다.

예를 들어 debug는 아래를 강제합니다.

- 재현 조건
- 관찰 증상
- 가설 2~3개
- 검증 순서

반면 write는 독자/톤/분량 기준을 먼저 줘야 재작업이 줄어듭니다.

## 3) 모델 라우팅보다 출력 계약이 더 중요하다

많은 팀이 모델 선택에 시간을 쓰지만, 실제 차이는 출력 계약에서 납니다.

- 결과 형식(체크리스트/코드/요약)
- 길이 제한
- 반드시 포함할 항목
- 금지 항목

출력 계약이 있으면 모델이 바뀌어도 품질이 크게 흔들리지 않습니다.

## 4) 로그를 남겨야 분류 규칙이 진화한다

처음 분류는 완벽하지 않습니다.  
그래서 실행 로그를 보고 오분류 패턴을 고쳐야 합니다.

저는 주간 리뷰 때 아래 3가지만 봅니다.

- 가장 많이 재시도된 유형
- 모델 재선택이 자주 발생한 유형
- 템플릿이 과하거나 부족했던 유형

이렇게 보면 규칙이 감각이 아니라 데이터로 개선됩니다.

## 체크리스트

- 작업 유형 4개를 팀/개인 기준으로 고정했는가
- 유형별 출력 계약(형식/길이/필수 항목)이 있는가
- 실행 로그에서 재시도 패턴을 추적하는가
- 주간 리뷰에서 규칙을 최소 1개씩 개선하는가

## 오늘의 메모

AI 활용의 병목은 모델 성능보다  
질문을 분류하는 운영 설계에서 먼저 풀립니다.
