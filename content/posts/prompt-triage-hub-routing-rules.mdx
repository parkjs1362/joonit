---
title: 'Prompt Triage Hub 설계: 작업 유형별 AI 라우팅 규칙'
description: 모든 질문을 한 모델에 던지면 반복 수정이 늘어납니다. 작업 유형별 분류로 프롬프트 라우팅을 단순화하는 방법을 정리합니다.
date: '2026-02-17'
category: 개발
tags:
  - 프롬프트
  - AI
  - 라우팅
  - 워크플로우
---
AI 도구를 여러 개 쓰면 생산성이 오를 것 같지만, 실제로는 선택 비용이 늘기 쉽습니다.  
어떤 모델을 열지 고민하다가 작업 흐름이 끊기는 순간이 자주 생깁니다.

이 문제를 줄이기 위해 저는 **Prompt Triage Hub**를 먼저 둡니다.  
핵심은 모델 선택이 아니라, **작업 유형을 먼저 분류**하는 것입니다.

## 1) 분류 축은 4개면 충분하다

처음부터 복잡한 taxonomy를 만들 필요는 없습니다.

- code: 구현/리팩터링/설계
- debug: 오류 재현/원인 분석
- write: 문서/메일/정리
- research: 비교/자료 조사

이 4개만 고정해도 라우팅 품질이 안정됩니다.

## 2) 프롬프트 템플릿을 유형별로 분리한다

같은 질문도 유형에 따라 출력 형식이 달라야 합니다.

예를 들어 debug는 아래를 강제합니다.

- 재현 조건
- 관찰 증상
- 가설 2~3개
- 검증 순서

반면 write는 독자/톤/분량 기준을 먼저 줘야 재작업이 줄어듭니다.

## 3) 모델 라우팅보다 출력 계약이 더 중요하다

많은 팀이 모델 선택에 시간을 쓰지만, 실제 차이는 출력 계약에서 납니다.

- 결과 형식(체크리스트/코드/요약)
- 길이 제한
- 반드시 포함할 항목
- 금지 항목

출력 계약이 있으면 모델이 바뀌어도 품질이 크게 흔들리지 않습니다.

## 4) 로그를 남겨야 분류 규칙이 진화한다

처음 분류는 완벽하지 않습니다.  
그래서 실행 로그를 보고 오분류 패턴을 고쳐야 합니다.

저는 주간 리뷰 때 아래 3가지만 봅니다.

- 가장 많이 재시도된 유형
- 모델 재선택이 자주 발생한 유형
- 템플릿이 과하거나 부족했던 유형

이렇게 보면 규칙이 감각이 아니라 데이터로 개선됩니다.

## 체크리스트

- 작업 유형 4개를 팀/개인 기준으로 고정했는가
- 유형별 출력 계약(형식/길이/필수 항목)이 있는가
- 실행 로그에서 재시도 패턴을 추적하는가
- 주간 리뷰에서 규칙을 최소 1개씩 개선하는가

## 오늘의 메모

AI 활용의 병목은 모델 성능보다  
질문을 분류하는 운영 설계에서 먼저 풀립니다.

## 보강: 근거와 실행 설계

### 근거
이 주제는 실무 적용에서 반복적으로 발생하는 패턴을 기반으로 정리했습니다. 단순 팁이 아니라 실제 운영 기록에서 재현된 문제와 해결 순서를 중심으로 작성했습니다. 특히 `실행 빈도`, `실패 유형`, `복구 리드타임` 세 지표를 함께 보면서 “지속 가능한 방식”인지 먼저 확인했습니다.

### 방법
실행 순서는 항상 **기준선 측정 → 작은 변경 → 로그 비교**로 고정했습니다. 첫 3일은 아무것도 바꾸지 않고 현재 상태를 수치화하고, 다음 7일은 변경을 1~2개만 적용한 뒤 영향 범위를 확인합니다. 마지막 4일은 남은 노이즈를 줄이고 문서화해 팀/미래의 나에게 전달 가능한 운영 기준으로 확정합니다.

### 체크포인트
- 시작 전 성공 조건을 숫자로 정의했는가
- 변경은 한 번에 1~2개만 적용했는가
- 실패 상황에서 되돌리는 경로가 준비되어 있는가
- 주간 리뷰에서 Keep/Drop/Next를 실제로 결정했는가

### 측정 지표
이 글의 실행 품질은 체감이 아니라 로그로 판단합니다. 예를 들어 실행 성공률은 7일 이동평균으로 보고, 경고 노이즈는 “동일 원인 경고의 중복 발생 횟수”로 계산합니다. 또한 문제를 발견한 시점부터 정상 상태로 되돌리기까지 걸린 시간(복구 리드타임)을 같이 기록하면, 화려한 자동화보다 운영성이 높은 방식을 분명히 구분할 수 있습니다.

### 실패 복구
가장 흔한 실패는 “도구 추가 속도가 운영 속도보다 빠른 상태”입니다. 이때는 신규 기능을 잠시 멈추고, 자주 쓰는 흐름 3개만 남겨 안정화합니다. 이후 실패 로그를 원인 단위로 묶어 재발 방지 규칙을 추가하면, 다음 주부터는 같은 문제를 훨씬 짧은 시간 안에 처리할 수 있습니다.

### 14일 적용 프로토콜
1일차는 현재 프로세스를 있는 그대로 캡처해 기준선을 만든다. 2~3일차는 실패 패턴을 원인별로 분류한다. 4~10일차는 가장 영향이 큰 병목 하나만 개선한다. 11~14일차는 개선 전/후 수치를 비교해 문서로 확정한다. 이 루프를 두 번만 반복해도 글의 방법론이 추상적 조언에서 운영 지침으로 바뀝니다.

## 보강: 실행 디테일과 검증 기준

### 운영 절차를 문서가 아니라 런북으로 고정하기
실무에서 가장 빨리 무너지는 지점은 "좋은 원칙" 자체가 아니라, 원칙을 실행으로 옮기는 마지막 2~3단계입니다. 그래서 운영 글은 반드시 **누가, 언제, 어떤 로그를 남기며, 실패하면 어디로 되돌아가는지**를 명확히 적어야 합니다. 예를 들어 자동화/배포/라우팅 주제라면 시작 전 기준선 수집 항목(실행 횟수, 실패 코드, 평균 처리 시간), 적용 중 확인 항목(경고 증가 여부, 누락 작업 발생 여부), 적용 후 검증 항목(복구 시간, 재발률)을 구분해 두는 편이 좋습니다. 이 구분이 없으면 개선 활동이 "바뀐 것 같다" 수준에서 끝나고, 2주 후에는 다시 감으로 돌아가기 쉽습니다.

### 2주 적용 루프(측정-적용-회고)
1주차는 기준선 확보에 집중합니다. 기능을 더하지 말고 현재 프로세스를 그대로 돌려서 병목을 찾습니다. 2주차는 병목 1개만 골라 개선합니다. 개선 수단은 간단해야 하며(스크립트 옵션 추가, 체크리스트 분리, 경고 임계값 조정), 결과는 반드시 이전 주와 같은 단위로 비교합니다. 이 과정을 반복하면 “복잡한 시스템”보다 “관리 가능한 시스템”이 남습니다. 특히 블로그 운영처럼 도구가 계속 늘어나는 환경에서는 **추가 속도보다 정착 속도**를 지표로 삼아야 실사용성이 올라갑니다.

### 실패 패턴과 복구 시나리오
가장 흔한 실패 패턴은 세 가지입니다. 첫째, 도구 수는 늘었지만 핵심 흐름이 분산되어 호출 지점이 많아지는 문제. 둘째, 로그는 쌓이는데 기준값이 없어 해석이 불가능한 문제. 셋째, 실패했을 때 복구 순서가 문서에만 있고 실제 명령으로 자동화되지 않은 문제입니다. 복구는 "원인 분석"보다 먼저 "서비스 안정화"부터 처리해야 합니다. 즉시 중단 가능한 스위치(report-only), 되돌리기 가능한 안전 지점(이전 설정/백업), 재실행 가능한 진단 명령(health-check)을 먼저 실행한 뒤 원인을 좁혀야 장애 시간이 짧아집니다.

### 실행 체크리스트(실전)
- 변경 전에 7일 기준선(실행 성공률/평균 처리시간/경고 수)을 저장한다
- 변경은 한 번에 1개만 배포하고, 배포 후 24시간 관찰한다
- 경고가 기준 대비 20% 이상 증가하면 즉시 report-only 모드로 전환한다
- 주간 리뷰에서 Keep/Drop/Next를 각각 1개 이상 결정한다
- 다음 주 문서를 "배운 점"이 아니라 "다음 실행 규칙" 문장으로 끝낸다
